{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing ARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTION 1: Importing libraries and define static variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from math import *\n",
    "from random import randint\n",
    "# FOR PLOT\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# FOR CHI-SQUARE - MUTUAL INFORMATION - RELIEF\n",
    "import sklearn_relief as relief\n",
    "\n",
    "# FOR CROSS FOLD VALIDATION\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "# FOR TESTING ON LOGISTIC REGRESSION\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "# where all utilities are defined\n",
    "import utils\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTION 2: Defining binary functions in functions.py\n",
    "## ACTION 3: Dataset generation, pair generation and sampling in data_generation.py\n",
    "## ACTION 4: Defining utilities for chi2 and mutual information in ch2_mi.py\n",
    "## ACTION 5: Defining baseline logistic regression in baseline.py\n",
    "## ACTION 6: Defining ARI in ari.py\n",
    "## ACTION 7: Function to validate ARI stability wrt sample size - order relevanceÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_score_latex(dimension,row_name,scores_means,scores_std):\n",
    "    latex_line=\"\"\n",
    "    for i in range(dimension):\n",
    "        latex_line+=\" & \"+str(round(scores_means[i],2))+\"-\"+str(round(scores_std[i],2))\n",
    "    print(row_name+latex_line+\"\\\\\\\\\")\n",
    "    return True\n",
    "\n",
    "def get_ari(dataset,dimension,sample_size):  #return a list of score per feature\n",
    "    list_of_attributes=[]\n",
    "    for i in range(dimension):   # index of attributes\n",
    "        list_of_attributes.append(i)    \n",
    "    ari_scores = [0]*dimension   # ari is all 0 to start with\n",
    "    list_of_pairs = all_pairs(dataset)\n",
    "    ari_scores = select_features_ars(dimension,list_of_attributes,dataset,list_of_pairs)\n",
    "    return ari_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTION 9: Function to compare ARI - chi-square - mutual information - relief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_score_on_dataset(filename,number_of_test,sample_ratio):\n",
    "    dataset, X, y, dimension = load_dataset(filename)\n",
    "    #INFO\n",
    "    dataset_size=dataset.shape[0]\n",
    "    print(\"****INFORMATION ON INITIAL DATA *******\")\n",
    "    print(\"dataset:\",filename,\"size:\",dataset_size,\"dimension:\",dimension)\n",
    "\n",
    "    #list of attribute as indices: 0, 1, ...\n",
    "    list_of_attributes=[]\n",
    "    attribute_names=[]\n",
    "    for i in range(dimension):\n",
    "        list_of_attributes.append(i)\n",
    "        attribute_names.append(\"a\"+str(i+1))\n",
    "    #print(attribute_names)\n",
    "    \n",
    "    sample_size = int(dataset_size*sample_ratio)\n",
    "    print(sample_size)\n",
    "    mean_ari_scores    = [0]*dimension\n",
    "    mean_chi_scores    = [0]*dimension\n",
    "    mean_mut_scores    = [0]*dimension\n",
    "    mean_relief_scores = [0]*dimension\n",
    "    #print(sample_size)\n",
    "\n",
    "    for u in range(number_of_test):\n",
    "        sample_set = generate_sample_set(dataset,sample_size)\n",
    "        list_of_pairs = all_pairs(sample_set)\n",
    "        #print(len(list_of_pairs))\n",
    "        ari_scores = select_features_ars(dimension,list_of_attributes,sample_set,list_of_pairs)\n",
    "        #print(ars_scores)\n",
    "# NOT SURE WE TEST ON THE SAME SET BECAUSE OF TEST_SIZE PARAM\n",
    "        X_train_enc, y_train_enc, X_test_enc=prepare_all(X, y, test_size=sample_ratio, random_state=1)\n",
    "        X_train_chi, X_test_chi, fs_chi = select_all_features_chi2(X_train_enc, y_train_enc, X_test_enc)\n",
    "        X_train_mut, X_test_mut, fs_mut = select_all_features_mutual(X_train_enc, y_train_enc, X_test_enc)\n",
    "    \n",
    "    # RELIEF\n",
    "        relief_scores = relief.Relief(n_features=dimension) # we check all attributes\n",
    "        my_transformed_matrix = relief_scores.fit_transform(X_train_enc,y_train_enc)\n",
    "\n",
    "# NORMALIZATION FACTORS - All scores are normalized +0.001 to avoid division by 0\n",
    "        Z_ari    = sum(ari_scores) + 0.001\n",
    "        Z_chi    = sum(fs_chi.scores_) + 0.001\n",
    "        Z_mi     = sum(fs_mut.scores_) + 0.001\n",
    "        Z_relief = sum(relief_scores.w_) + 0.001\n",
    "        \n",
    "#UPDATE MEAN SCORES BY ADDING NORMALIZED SCORES IN [0,1]  \n",
    "        for j in range(dimension):\n",
    "            mean_ari_scores[j]   += ari_scores[j]/(Z_ari)\n",
    "            mean_chi_scores[j]   += fs_chi.scores_[j]/(Z_chi)\n",
    "            mean_mut_scores[j]   += fs_mut.scores_[j]/(Z_mi)\n",
    "            mean_relief_scores[j]+= relief_scores.w_[j]/(Z_relief)\n",
    "\n",
    "    mean_ari_scores    = [a*(1/number_of_test) for a in mean_ari_scores]\n",
    "    mean_chi_scores    = [a*(1/number_of_test) for a in mean_chi_scores]\n",
    "    mean_mut_scores    = [a*(1/number_of_test) for a in mean_mut_scores]\n",
    "    mean_relief_scores = [a*(1/number_of_test) for a in mean_relief_scores]\n",
    "    \n",
    "    ari_scores_std  = [np.array(mean_ari_scores[j]).std() for j in range(dimension)]\n",
    "    chi_scores_std  = [np.array(mean_chi_scores[j]).std() for j in range(dimension)]\n",
    "    mut_scores_std  = [np.array(mean_mut_scores[j]).std() for j in range(dimension)]\n",
    "    relief_scores_std  = [np.array(mean_relief_scores[j]).std() for j in range(dimension)]\n",
    "\n",
    "    pyplot.title(\"ARI\")\n",
    "    pyplot.bar(attribute_names, mean_ari_scores)\n",
    "    pyplot.show()\n",
    "\n",
    "    pyplot.title(\"CHI-SQUARE\")\n",
    "    pyplot.bar(attribute_names, mean_chi_scores)\n",
    "    pyplot.show()\n",
    "\n",
    "    pyplot.title(\"MUTUAL INFORMATION\")\n",
    "    pyplot.bar(attribute_names, mean_mut_scores)\n",
    "    pyplot.show()\n",
    "\n",
    "    pyplot.title(\"RELIEF\")\n",
    "    pyplot.bar(attribute_names, mean_relief_scores)\n",
    "    pyplot.show()\n",
    "# PREPARE FOR LATEX\n",
    "    display_score_latex(dimension,\"   ari\",mean_ari_scores,ari_scores_std)\n",
    "    display_score_latex(dimension,\"  chi2\",mean_chi_scores,chi_scores_std)\n",
    "    display_score_latex(dimension,\"    mi\",mean_mut_scores,mut_scores_std)\n",
    "    display_score_latex(dimension,\"relief\",mean_relief_scores,relief_scores_std)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTION 10: Comparing feature score methods values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6-primary-tumor.data-no_missing.csv\n",
      "1-monks-1.csv\n",
      ".DS_Store\n",
      "2-monks-2.csv\n",
      "4-breast-cancer.csv\n",
      "7-mushroom.csv\n",
      "5-hiv.csv\n",
      "3-monks-3.csv\n"
     ]
    }
   ],
   "source": [
    "number_of_test = 10\n",
    "sample_ratio   = 0.8   #THERE IS AN ISSUE HERE AS I DO NOT UNDERSTAND TRAIN/TEST SAMPLE_RATIO\n",
    "number_of_folds=10\n",
    "tested_folder=\"datasets-tested/\"\n",
    "for filename in os.listdir(tested_folder):\n",
    "    print(filename)\n",
    "    if filename==\".DS_Store\":\n",
    "        continue\n",
    "    #compare_score_on_dataset(tested_folder+filename,number_of_test,sample_ratio)\n",
    "    #dataset, X, y, dimension = load_dataset(tested_folder+filename)\n",
    "    #accuracy=baseline_for_categorical_with_all(X, y,number_of_folds)\n",
    "    #print(filename,\"- accuracy baseline:\",round(accuracy,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTION 11: Comparing feature relevance score effectiveness on logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_categorical_dataset(filename,k):  \n",
    "    dataset, X, y, dimension = load_dataset(filename)\n",
    "    print(\"data shape:\",X.shape, \"dimension\", dimension,\"nb of used best features:\",k)\n",
    "    FOLDS=10\n",
    "    list_of_attributes=[]\n",
    "    for i in range(dimension):\n",
    "        list_of_attributes.append(i)\n",
    "    acc_baseline_all_features=baseline_for_categorical_with_all(X,y,FOLDS) #10 fold cross valid\n",
    "    list_of_accuracy_ars=[]\n",
    "    list_of_accuracy_chi2=[]\n",
    "    list_of_accuracy_mi=[]\n",
    "    list_of_accuracy_relief=[]\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True)\n",
    "    X_enc=prepare_input(X)\n",
    "    y_enc=prepare_target(y)\n",
    "    for train, test in skf.split(X,y):\n",
    "        X_train_chi2_enc, X_test_chi2_enc, _ = select_k_features_chi2(X_enc[train], y_enc[train], X_enc[test], k)\n",
    "        X_train_mi_enc, X_test_mi_enc, _ = select_k_features_mi(X_enc[train], y_enc[train], X_enc[test], k)\n",
    "        r = relief.Relief(n_features=k)\n",
    "        X_train_relief_enc = r.fit_transform(X_enc[train], y_enc[train])\n",
    "        X_test_relief_enc  = r.fit_transform(X_enc[test], y_enc[test])\n",
    "    # ARS feature selection\n",
    "        #create A : list of attribute as index 0, 1, ...\n",
    "        list_of_attributes=[]\n",
    "        for i in range(dimension):\n",
    "            list_of_attributes.append(i)\n",
    "        sample_set = dataset[train]\n",
    "        list_of_pairs = all_pairs(sample_set)\n",
    "        ars_scores = select_features_ars(dimension,list_of_attributes,sample_set,list_of_pairs)\n",
    "        s = np.array(ars_scores)\n",
    "        sort_index = np.argsort(s)\n",
    "        sort_index=np.flipud(sort_index)\n",
    "        #print(\"ars index\",sort_index)\n",
    "        #print(\"ars scores:\",ars_scores)\n",
    "        # transform the dataset to keep only the k relevant features if there is 0 relevance\n",
    "        # k should be redefined for ARI\n",
    "        #print(sort_index[0:k],s[sort_index[0:k][k-1]])\n",
    "        if s[sort_index[0:k][k-1]]==0:\n",
    "            k=k-1   #removing the last guy if relevance 0 like in monks1\n",
    "        X_train_ars_enc = np.delete(X_enc[train], sort_index[0:k],axis=1)\n",
    "        X_test_ars_enc  = np.delete(X_enc[test],sort_index[0:k],axis=1)\n",
    "        a=accuracy(X_train_ars_enc,y[train],X_test_ars_enc,y[test])\n",
    "        list_of_accuracy_ars.append(a)  \n",
    "    # CHI2 feature selection\n",
    "        a=accuracy(X_train_chi2_enc,y_enc[train],X_test_chi2_enc,y_enc[test])\n",
    "        list_of_accuracy_chi2.append(a)\n",
    "    # MI feature selection\n",
    "        a=accuracy(X_train_mi_enc,y_enc[train],X_test_mi_enc,y_enc[test])\n",
    "        list_of_accuracy_mi.append(a)\n",
    "    # RELIEF\n",
    "        a=accuracy(X_train_relief_enc,y_enc[train],X_test_relief_enc,y_enc[test])\n",
    "        list_of_accuracy_relief.append(a)\n",
    "        \n",
    "    acc_ars   = mean(list_of_accuracy_ars)\n",
    "    acc_chi2  = mean(list_of_accuracy_chi2)\n",
    "    acc_mi    = mean(list_of_accuracy_mi)\n",
    "    acc_relief= mean(list_of_accuracy_relief)\n",
    "    \n",
    "    return(acc_baseline_all_features,acc_ars,acc_chi2,acc_mi,acc_relief)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tested_folder=\"datasets-tested/\"\n",
    "for filename in sorted((os.listdir(tested_folder))):\n",
    "    print(filename)\n",
    "    if filename==\".DS_Store\":\n",
    "        continue\n",
    "    dataset = read_csv(tested_folder+filename, header=None)\n",
    "    data = dataset.values\n",
    "    dimension=data.shape[1] - 1\n",
    "    k_list= [dimension//5, dimension//4,dimension//3, dimension//2, dimension-1]\n",
    "    k_list=list(set(k_list))\n",
    "    k_list=[4]\n",
    "    print(\"Tested dataset:\",filename,\"- dimension:\",dimension,\" - k best dimension tested:\",k_list)\n",
    "    for num in k_list:\n",
    "        if num>=1:\n",
    "         acc_baseline_all_features,acc_ars,acc_chi2,acc_mi,acc_relief=test_categorical_dataset(tested_folder+filename,num)\n",
    "         print(\"baseline:\",acc_baseline_all_features,\" - ARI:\",acc_ars,\" - chi2:\",acc_chi2,\" - mi:\",acc_mi,\" - relief:\",acc_relief)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monks2-ari.csv - data shape: (601, 4) dimension 4\n",
      "0.6572404371584699\n",
      "monks2-chi2.csv - data shape: (601, 4) dimension 4\n",
      "0.6539071038251366\n",
      "monks2-mi.csv - data shape: (601, 4) dimension 4\n",
      "0.6572404371584699\n",
      "monks2-relief.csv - data shape: (601, 4) dimension 4\n",
      "0.6572404371584699\n"
     ]
    }
   ],
   "source": [
    "tested_folder=\"datasets-refactor/Monks2/\"\n",
    "FOLDS=10\n",
    "for filename in sorted((os.listdir(tested_folder))):\n",
    "    if filename==\".DS_Store\":\n",
    "        continue\n",
    "    dataset, X, y, dimension = load_dataset(tested_folder+filename)\n",
    "    print(filename,\"- data shape:\",X.shape, \"dimension\", dimension)\n",
    "    \n",
    "    #print(\"Tested dataset:\",filename,\"- dimension:\",dimension,\" - k best dimension tested:\",k_list)\n",
    "    print(baseline_for_categorical_with_all(X, y,FOLDS))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
