{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing ARI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTION 1: Importing libraries and define static variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from math import *\n",
    "from random import randint\n",
    "# FOR PLOT\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# FOR CHI-SQUARE - MUTUAL INFORMATION - RELIEF\n",
    "import sklearn_relief as relief\n",
    "\n",
    "# FOR CROSS FOLD VALIDATION\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "# FOR TESTING ON LOGISTIC REGRESSION\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "# where all utilities are defined\n",
    "import utils\n",
    "from utils import *\n",
    "\n",
    "DATASET_GENERAL = \"datasets-tested/dataset_general.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTION 2: Defining binary functions in functions.py\n",
    "## ACTION 3: Dataset generation, pair generation and sampling in data_generation.py\n",
    "## ACTION 4: Defining utilities for chi2 and mutual information in ch2_mi.py\n",
    "## ACTION 5: Defining baseline logistic regression in baseline.py\n",
    "## ACTION 6: Defining ARI in ari.py\n",
    "## ACTION 7: Function to validate ARI stability wrt sample size - order relevanceÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_score_latex(dimension,row_name,ari_scores_means,ari_scores_std):\n",
    "    latex_line=\"\"\n",
    "    for i in range(dimension):\n",
    "        latex_line+=\" & \"+str(round(ari_scores_means[i],2))+\"-\"+str(round(ari_scores_std[i],2))\n",
    "    print(\"sample \"+row_name+latex_line+\"\\\\\\\\\")\n",
    "    return True\n",
    "\n",
    "def get_ari(dataset,dimension,sample_size):  #return a list of score per feature\n",
    "    list_of_attributes=[]\n",
    "    for i in range(dimension):   # index of attributes\n",
    "        list_of_attributes.append(i)    \n",
    "    ari_scores = [0]*dimension   # ari is all 0 to start with\n",
    "    list_of_pairs = all_pairs(dataset)\n",
    "    ari_scores = select_features_ars(dimension,list_of_attributes,dataset,list_of_pairs)\n",
    "    return ari_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTION 8: Validate ARI stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for sample_size in sample_size_list:\n",
    "    mean_ari_scores = [0]*dimension\n",
    "    feature_score_list=[]\n",
    "    for i in range(dimension): #create a list of list of scores per feature\n",
    "        feature_score_list.append([])\n",
    "    for u in range(number_of_test):\n",
    "        #uncomment the line below when running ari with test_stability_ari\n",
    "        create_categorical_dataset(filename,f,dimension,sample_size,categorical_range)\n",
    "        dataset, X, y, dimension = load_dataset(filename)\n",
    "        ari_scores = get_ari(dataset,dimension,sample_size)\n",
    "        for j in range(dimension): #accumulate the scores\n",
    "            mean_ari_scores[j]+=ari_scores[j]\n",
    "            feature_score_list[j].append(ari_scores[j])\n",
    "    #print(len(feature_score_list[0]))       \n",
    "  #  mean_ari_scores = [a*(1/number_of_test) for a in mean_ari_scores]\n",
    "    ari_scores_means = [mean(feature_score_list[j]) for j in range(dimension)]\n",
    "    ari_scores_std  = [np.array(feature_score_list[j]).std() for j in range(dimension)]\n",
    "    #print(ari_scores_std)\n",
    "    display_score_latex(dimension,str(sample_size),ari_scores_means,ari_scores_std) \n",
    "'''    \n",
    "    \n",
    "def validate_stability(dimension,categorical_range,f,number_of_test,sample_size_list):\n",
    "    for sample_size in sample_size_list:\n",
    "        mean_ari_scores = [0]*dimension\n",
    "        feature_score_list=[]\n",
    "        for i in range(dimension): #create a list of list of scores per feature\n",
    "            feature_score_list.append([])\n",
    "        for u in range(number_of_test):\n",
    "            create_categorical_dataset(filename,f,dimension,sample_size,categorical_range)\n",
    "            dataset, X, y, dimension = load_dataset(filename)\n",
    "            ari_scores = get_ari(dataset,dimension,sample_size)\n",
    "            for j in range(dimension): #accumulate the scores\n",
    "                mean_ari_scores[j]+=ari_scores[j]\n",
    "                feature_score_list[j].append(ari_scores[j])\n",
    "        ari_scores_means = [mean(feature_score_list[j]) for j in range(dimension)]\n",
    "        ari_scores_std  = [np.array(feature_score_list[j]).std() for j in range(dimension)]\n",
    "        display_score_latex(dimension,str(sample_size),ari_scores_means,ari_scores_std)\n",
    "    return\n",
    "\n",
    "dimension = 10\n",
    "categorical_range = 1 #1 means binary - range of values for feature\n",
    "#f = g1_array\n",
    "\n",
    "size_of_X=int(pow((categorical_range+1),dimension))\n",
    "\n",
    "number_of_test = 1\n",
    "sample_size_list = [300,400,500]\n",
    "sample_size_list = [50]\n",
    "\n",
    "filename=DATASET_GENERAL\n",
    "print(\"dimension:\",dimension, \"- size of X:\",size_of_X, \" - number of test:\",number_of_test) \n",
    "\n",
    "list_of_functions=[g1_array,g2_array,g3_array,g4_array,g5_array,g6_array,g7_array,g8_array]\n",
    "list_of_functions=[g1_array]\n",
    "for f in list_of_functions:\n",
    "    print(\"function:\",str(f))\n",
    "    validate_stability(dimension,categorical_range,f,number_of_test,sample_size_list)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTION 9-old: Function to compare ARS - chi-square - mutual information - relief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old\n",
    "NO_OF_FS = 4\n",
    "ARI=0\n",
    "CHI=1\n",
    "MI=2\n",
    "RELIEF=3\n",
    "Z_EPSILON=0.001 # values added to denominator to avoid division by 0\n",
    "\n",
    "def _old_compare_score_on_synthetic_dataset(filename,number_of_test):\n",
    "    \n",
    "    fs =[\n",
    "     {\"latex\": \"   ari\", \"chart\": \"ARI\"},\n",
    "     {\"latex\": \"  chi2\", \"chart\": \"CHI SQUARE\"},\n",
    "     {\"latex\": \"    mi\", \"chart\": \"MUTUAL INFORMATION\"},\n",
    "     {\"latex\": \"relief\", \"chart\": \"RELIEF\"}]\n",
    "\n",
    "       \n",
    "    dataset, X, y, dimension = load_dataset(filename)\n",
    "    #INFO\n",
    "    dataset_size=dataset.shape[0]\n",
    "    print(\"****INFORMATION ON INITIAL DATA *******\")\n",
    "    print(\"dataset:\",filename,\"size:\",dataset_size,\"dimension:\",dimension, \"number of test:\", number_of_test)\n",
    "    \n",
    "    feature_score_list_ari = []\n",
    "    for i in range(dimension): #create a list of list of scores per feature\n",
    "        feature_score_list_ari.append([])\n",
    "                \n",
    "    #list of attribute as indices: 0, 1, ...\n",
    "    list_of_attributes=[]\n",
    "    attribute_names=[]\n",
    "    for i in range(dimension):\n",
    "        list_of_attributes.append(i)\n",
    "        attribute_names.append(\"a\"+str(i+1))\n",
    "    #print(attribute_names)\n",
    "    \n",
    "    sample_size = dataset_size\n",
    "    print(sample_size)\n",
    "\n",
    "    #create a 3D array to store the score for each feature, each dimension of the data, and each test\n",
    "    feature_score_list = np.empty((NO_OF_FS, dimension, number_of_test))    \n",
    "    \n",
    "    '''\n",
    "    feature_score_list = [[foo for i in range(10)] for j in range(10)]\n",
    "    for i in range(NO_OF_FS): #create a list to store scores for each type\n",
    "        feature_score_list.append([])\n",
    "        for j in range(dimension): #create a list to store scores per feature\n",
    "    '''    \n",
    "\n",
    "    mean_ari_scores    = [0]*dimension\n",
    "    mean_chi_scores    = [0]*dimension\n",
    "    mean_mut_scores    = [0]*dimension\n",
    "    mean_relief_scores = [0]*dimension\n",
    "    \n",
    "    \n",
    "#FOR EACH TEST\n",
    "    for u in range(number_of_test):\n",
    "        sample_set = generate_sample_set(dataset,sample_size)\n",
    "        list_of_pairs = all_pairs(sample_set)\n",
    "        #print(len(list_of_pairs))\n",
    "        ari_scores = select_features_ars(dimension,list_of_attributes,sample_set,list_of_pairs)\n",
    "        #print(ars_scores)\n",
    "\n",
    "        # NOT SURE WE TEST ON THE SAME SET BECAUSE OF TEST_SIZE PARAM\n",
    "        #SLIM - 21/06 - removed TEST_SIZE PARAM\n",
    "        \n",
    "        fs_chi = calculate_chi2_scores(X, y)\n",
    "        fs_mut = calculate_mi_scores(X, y)\n",
    "    \n",
    "    # RELIEF\n",
    "        relief_scores = relief.Relief(n_features=dimension) # we check all attributes\n",
    "        my_transformed_matrix = relief_scores.fit_transform(X,y)\n",
    "\n",
    "# NORMALIZATION FACTORS - All scores are normalized +0.001 to avoid division by 0\n",
    "        Z_ari    = sum(ari_scores) + 0.001\n",
    "        Z_chi    = sum(fs_chi.scores_) + 0.001\n",
    "        Z_mi     = sum(fs_mut.scores_) + 0.001\n",
    "        Z_relief = sum(relief_scores.w_) + 0.001\n",
    "\n",
    "#STORE EACH SCORE \n",
    "        #store the score for CHI for all dimensions in test number u\n",
    "        feature_score_list[ARI,:,u:u+1] = np.reshape(ari_scores, [dimension,1])/ (sum(ari_scores) + Z_EPSILON)\n",
    "        feature_score_list[CHI,:,u:u+1] = np.reshape(fs_chi.scores_, [dimension,1])/ (sum(fs_chi.scores_) + Z_EPSILON)\n",
    "        feature_score_list[MI,:,u:u+1]  = np.reshape(fs_mut.scores_, [dimension,1])/ (sum(fs_mut.scores_) + Z_EPSILON)      \n",
    "        feature_score_list[RELIEF,:,u:u+1]  = np.reshape(relief_scores.w_, [dimension,1])/ (sum(relief_scores.w_) + Z_EPSILON)      \n",
    "        \n",
    "#UPDATE MEAN SCORES BY ADDING NORMALIZED SCORES IN [0,1]  \n",
    "        for j in range(dimension):\n",
    "            mean_ari_scores[j]   += ari_scores[j]/(Z_ari)\n",
    "            mean_chi_scores[j]   += fs_chi.scores_[j]/(Z_chi)            \n",
    "            mean_mut_scores[j]   += fs_mut.scores_[j]/(Z_mi)\n",
    "            mean_relief_scores[j]+= relief_scores.w_[j]/(Z_relief)\n",
    "            feature_score_list_ari[j].append(ari_scores[j]/Z_ari)            \n",
    "\n",
    "    mean_ari_scores    = [a*(1/number_of_test) for a in mean_ari_scores]\n",
    "    mean_chi_scores    = [a*(1/number_of_test) for a in mean_chi_scores]\n",
    "    mean_mut_scores    = [a*(1/number_of_test) for a in mean_mut_scores]\n",
    "    mean_relief_scores = [a*(1/number_of_test) for a in mean_relief_scores]\n",
    "    \n",
    "    ari_scores_std  = [np.array(feature_score_list_ari[j]).std() for j in range(dimension)] \n",
    "    \n",
    "\n",
    "    pyplot.title(\"ARI\")\n",
    "    pyplot.bar(attribute_names, mean_ari_scores)\n",
    "    pyplot.show()\n",
    "\n",
    "    pyplot.title(\"CHI-SQUARE\")\n",
    "    pyplot.bar(attribute_names, mean_chi_scores)\n",
    "    pyplot.show()\n",
    "\n",
    "    pyplot.title(\"MUTUAL INFORMATION\")\n",
    "    pyplot.bar(attribute_names, mean_mut_scores)\n",
    "    pyplot.show()\n",
    "\n",
    "    pyplot.title(\"RELIEF\")\n",
    "    pyplot.bar(attribute_names, mean_relief_scores)\n",
    "    pyplot.show()\n",
    "    \n",
    "    mean = np.mean(feature_score_list, axis=2)\n",
    "    std  = np.std(feature_score_list, axis=2)\n",
    "    \n",
    "    for i in range(len(fs)):\n",
    "        pyplot.title(fs[i]['chart'])\n",
    "        pyplot.bar(attribute_names, mean[i])\n",
    "        pyplot.show()\n",
    "    \n",
    "    \n",
    "# PREPARE FOR LATEX\n",
    "\n",
    "    #display_score_latex(dimension,str(sample_size),ari_scores_means,ari_scores_std)\n",
    "    display_score_latex(dimension,\"     ari\",mean_ari_scores,ari_scores_std)\n",
    "    display_score_latex(dimension,\"   npari\",mean[ARI],std[ARI])\n",
    "    \n",
    "    display_score_latex(dimension,\"    chi2\",mean_chi_scores,std[CHI])\n",
    "    display_score_latex(dimension,\"  npchi2\",mean[CHI],std[CHI])\n",
    "    \n",
    "    display_score_latex(dimension,\"      mi\",mean_mut_scores,std[MI])\n",
    "    display_score_latex(dimension,\"    npmi\",mean[MI],std[MI])\n",
    "    \n",
    "    display_score_latex(dimension,\"  relief\",mean_relief_scores, std[RELIEF])\n",
    "    display_score_latex(dimension,\"nprelief\",mean[RELIEF],std[RELIEF])\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    for i in range(len(fs)):\n",
    "        display_score_latex(dimension, fs[i]['latex'] ,mean[i],std[i])\n",
    "        \n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTION 9: Function to compare ARS - chi-square - mutual information - relief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_OF_FS = 4 # no of feature selection to be tested\n",
    "ARI=0    # the indexes for the four features\n",
    "CHI=1\n",
    "MI=2\n",
    "RELIEF=3\n",
    "Z_EPSILON=0.001 # values added to denominator to avoid division by 0\n",
    "\n",
    "def compare_score_on_synthetic_dataset(filename,number_of_test):\n",
    "       \n",
    "    # the titles of the feature selections for the outputs in latex and chart\n",
    "    fs =[\n",
    "     {\"latex\": \"   ari\", \"chart\": \"ARI\"},\n",
    "     {\"latex\": \"  chi2\", \"chart\": \"CHI SQUARE\"},\n",
    "     {\"latex\": \"    mi\", \"chart\": \"MUTUAL INFORMATION\"},\n",
    "     {\"latex\": \"relief\", \"chart\": \"RELIEF\"}]\n",
    "       \n",
    "    dataset, X, y, dimension = load_dataset(filename)\n",
    "    #INFO\n",
    "    dataset_size=dataset.shape[0]\n",
    "    print(\"****INFORMATION ON INITIAL DATA *******\")\n",
    "    print(\"dataset:\",filename,\"size:\",dataset_size,\"dimension:\",dimension, \"number of test:\", number_of_test)\n",
    "    \n",
    "    #list of attribute as indices: 0, 1, ...\n",
    "    list_of_attributes=[]\n",
    "    attribute_names=[]\n",
    "    for i in range(dimension):\n",
    "        list_of_attributes.append(i)\n",
    "        attribute_names.append(\"a\"+str(i+1))\n",
    "    #print(attribute_names)\n",
    "    \n",
    "    sample_size = dataset_size\n",
    "    print(sample_size)\n",
    "\n",
    "    #create a 3D array to store the score for each feature, each dimension of the data, and each test\n",
    "    feature_score_list = np.empty((NO_OF_FS, dimension, number_of_test))    \n",
    "    \n",
    "#FOR EACH TEST\n",
    "    for u in range(number_of_test):\n",
    "        sample_set = generate_sample_set(dataset,sample_size)\n",
    "        list_of_pairs = all_pairs(sample_set)\n",
    "        #print(len(list_of_pairs))\n",
    "        ari_scores = select_features_ars(dimension,list_of_attributes,sample_set,list_of_pairs)\n",
    "        #print(ars_scores)\n",
    "\n",
    "        # NOT SURE WE TEST ON THE SAME SET BECAUSE OF TEST_SIZE PARAM\n",
    "        #SLIM - 21/06 - removed TEST_SIZE PARAM\n",
    "        fs_chi = calculate_chi2_scores(X, y)\n",
    "        fs_mut = calculate_mi_scores(X, y)\n",
    "    \n",
    "    # RELIEF\n",
    "        relief_scores = relief.Relief(n_features=dimension) # we check all attributes\n",
    "        my_transformed_matrix = relief_scores.fit_transform(X,y)\n",
    "\n",
    "#STORE EACH SCORE \n",
    "        #store the score for each feature selection techniques for all dimensions for each test (u)\n",
    "        # NORMALIZATION FACTORS - All scores are normalized +0.001 to avoid division by 0\n",
    "        feature_score_list[ARI,:,u:u+1] = np.reshape(ari_scores, [dimension,1])/ (sum(ari_scores) + Z_EPSILON)\n",
    "        feature_score_list[CHI,:,u:u+1] = np.reshape(fs_chi.scores_, [dimension,1])/ (sum(fs_chi.scores_) + Z_EPSILON)\n",
    "        feature_score_list[MI,:,u:u+1]  = np.reshape(fs_mut.scores_, [dimension,1])/ (sum(fs_mut.scores_) + Z_EPSILON)      \n",
    "        feature_score_list[RELIEF,:,u:u+1]  = np.reshape(relief_scores.w_, [dimension,1])/ (sum(relief_scores.w_) + Z_EPSILON)      \n",
    "        \n",
    "    mean = np.mean(feature_score_list, axis=2)\n",
    "    std  = np.std(feature_score_list, axis=2)    \n",
    "\n",
    "#DISPLAY THE BAR CHART PLOTS\n",
    "    for i in range(len(fs)):\n",
    "        pyplot.title(fs[i]['chart'])\n",
    "        pyplot.bar(attribute_names, mean[i])\n",
    "        pyplot.show()    \n",
    "    \n",
    "# PREPARE FOR LATEX      \n",
    "    for i in range(len(fs)):\n",
    "        display_score_latex(dimension, fs[i]['latex'] ,mean[i],std[i])\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTION 10: Comparing score methods - synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename=\"datasets-tested/primary-tumor.data-no_missing.csv\"\n",
    "#filename=\"datasets-tested/1-monks-1.csv\"\n",
    "filename=\"datasets-tested/dataset_general.csv\"\n",
    "sample_size = 50\n",
    "dimension = 10\n",
    "categorical_range = 3\n",
    "\n",
    "number_of_test = 2\n",
    "#sample_ratio   = 0.8  #THERE IS AN ISSUE HERE AS I DO NOT UNDERSTAND TRAIN/TEST SAMPLE_RATIO\n",
    "list_of_functions=[g1_array,g2_array,g3_array,g4_array,g5_array,g6_array,g7_array]\n",
    "list_of_functions=[g1_array]\n",
    "for f in list_of_functions:\n",
    "    print(\"comparing score - synthetic data - function:\",str(f))   \n",
    "    create_categorical_dataset(filename,g1_array,dimension,sample_size,categorical_range)\n",
    "    compare_score_on_synthetic_dataset(filename,number_of_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTION 11: Comparing feature relevance score effectiveness on logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_binary_dataset(filename,k): #comparing accuracies by running logistic regression on k best features\n",
    "    dataset, X, y, dimension = load_dataset(filename)\n",
    "    print(\"data shape:\",X.shape, \"dimension\", dimension,\"nb of best features:\",k)\n",
    "    FOLDS=2\n",
    "    list_of_attributes=[]\n",
    "    for i in range(dimension):\n",
    "        list_of_attributes.append(i)\n",
    "    acc_baseline_all_features=baseline_for_binary_with_all(X, y,FOLDS) #10 fold cross valid\n",
    "    list_of_accuracy_ars=[]\n",
    "    list_of_accuracy_chi2=[]\n",
    "    list_of_accuracy_mi=[]\n",
    "    list_of_accuracy_relief=[]\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True)\n",
    "    for train, test in skf.split(X,y):\n",
    "        #print(\"****************\")\n",
    "        X_train_chi2, X_test_chi2, _ = select_k_features_chi2(X[train], y[train], X[test], k)\n",
    "        X_train_mi, X_test_mi, _ = select_k_features_mi(X[train], y[train], X[test], k)\n",
    "        r = relief.Relief(n_features=k)\n",
    "        X_train_relief = r.fit_transform(X[train], y[train])\n",
    "        X_test_relief = r.fit_transform(X[test], y[test])              \n",
    "    # ARS feature selection\n",
    "        #create A : list of attribute as index 0, 1, ...\n",
    "        list_of_attributes=[]\n",
    "        for i in range(dimension):\n",
    "            list_of_attributes.append(i)\n",
    "        sample_set = dataset[train]\n",
    "        list_of_pairs = all_pairs(sample_set)\n",
    "        ars_scores = select_features_ars(dimension,list_of_attributes,sample_set,list_of_pairs)\n",
    "        s = numpy.array(ars_scores)\n",
    "        sort_index = np.argsort(s)\n",
    "        print(sort_index)\n",
    "        print(ars_scores)\n",
    "        #transform the dataset to keep only the k relevant features\n",
    "        \n",
    "        a=accuracy(X_train_ars,y[train],X_test_ars,y[test])\n",
    "        list_of_accuracy_ars.append(a)   \n",
    "    # CHI2\n",
    "        a=accuracy(X_train_chi2,y[train],X_test_chi2,y[test])\n",
    "        list_of_accuracy_chi2.append(a)\n",
    "        #print(\"  chi2\",a)   \n",
    "    # MI \n",
    "        a=accuracy(X_train_mi,y[train],X_test_mi,y[test])\n",
    "        list_of_accuracy_mi.append(a)\n",
    "        #print(\"    mi\",a)\n",
    "    # RELIEF\n",
    "        a=accuracy(X_train_relief,y[train],X_test_relief,y[test])\n",
    "        list_of_accuracy_relief.append(a)\n",
    "        #print(\"relief\",a)\n",
    "        \n",
    "    acc_ars   = mean(list_of_accuracy_ars)\n",
    "    acc_chi2  = mean(list_of_accuracy_chi2)\n",
    "    acc_mi    = mean(list_of_accuracy_mi)\n",
    "    acc_relief= mean(list_of_accuracy_relief)\n",
    "    return(acc_baseline_all_features,acc_ars,acc_chi2,acc_mi,acc_relief)\n",
    "    \n",
    "def test_categorical_dataset(filename,k):  \n",
    "    dataset, X, y, dimension = load_dataset(filename)\n",
    "    print(\"data shape:\",X.shape, \"dimension\", dimension,\"nb of used best features:\",k)\n",
    "    FOLDS=10\n",
    "    list_of_attributes=[]\n",
    "    for i in range(dimension):\n",
    "        list_of_attributes.append(i)\n",
    "    acc_baseline_all_features=baseline_for_categorical_with_all(X,y,FOLDS) #10 fold cross valid\n",
    "    list_of_accuracy_ars=[]\n",
    "    list_of_accuracy_chi2=[]\n",
    "    list_of_accuracy_mi=[]\n",
    "    list_of_accuracy_relief=[]\n",
    "    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True)\n",
    "    X_enc=prepare_input(X)\n",
    "    y_enc=prepare_target(y)\n",
    "    for train, test in skf.split(X,y):\n",
    "        X_train_chi2_enc, X_test_chi2_enc, _ = select_k_features_chi2(X_enc[train], y_enc[train], X_enc[test], k)\n",
    "        X_train_mi_enc, X_test_mi_enc, _ = select_k_features_mi(X_enc[train], y_enc[train], X_enc[test], k)\n",
    "        r = relief.Relief(n_features=k)\n",
    "        X_train_relief_enc = r.fit_transform(X_enc[train], y_enc[train])\n",
    "        X_test_relief_enc = r.fit_transform(X_enc[test], y_enc[test])\n",
    "    # ARS feature selection\n",
    "        #create A : list of attribute as index 0, 1, ...\n",
    "        list_of_attributes=[]\n",
    "        for i in range(dimension):\n",
    "            list_of_attributes.append(i)\n",
    "        sample_set = dataset[train]\n",
    "        list_of_pairs = all_pairs(sample_set)\n",
    "        ars_scores = select_features_ars(dimension,list_of_attributes,sample_set,list_of_pairs)\n",
    "        s = np.array(ars_scores)\n",
    "        sort_index = np.argsort(s)\n",
    "        sort_index=np.flipud(sort_index)\n",
    "        #print(\"ars index\",sort_index)\n",
    "        #print(\"ars scores:\",ars_scores)\n",
    "        # transform the dataset to keep only the k relevant features\n",
    "        X_train_ars_enc = np.delete(X_enc[train], sort_index[0:k],axis=1)\n",
    "        X_test_ars_enc  = np.delete(X_enc[test],sort_index[0:k],axis=1)\n",
    "        a=accuracy(X_train_ars_enc,y[train],X_test_ars_enc,y[test])\n",
    "        list_of_accuracy_ars.append(a)  \n",
    "    # CHI2 feature selection\n",
    "        a=accuracy(X_train_chi2_enc,y_enc[train],X_test_chi2_enc,y_enc[test])\n",
    "        list_of_accuracy_chi2.append(a)\n",
    "    # MI feature selection\n",
    "        a=accuracy(X_train_mi_enc,y_enc[train],X_test_mi_enc,y_enc[test])\n",
    "        list_of_accuracy_mi.append(a)\n",
    "    # RELIEF\n",
    "        a=accuracy(X_train_relief_enc,y_enc[train],X_test_relief_enc,y_enc[test])\n",
    "        list_of_accuracy_relief.append(a)\n",
    "        \n",
    "    acc_ars   = mean(list_of_accuracy_ars)\n",
    "    acc_chi2  = mean(list_of_accuracy_chi2)\n",
    "    acc_mi    = mean(list_of_accuracy_mi)\n",
    "    acc_relief= mean(list_of_accuracy_relief)\n",
    "    return(acc_baseline_all_features,acc_ars,acc_chi2,acc_mi,acc_relief)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_path=\"datasets-tested/\"\n",
    "file_to_be_tested=os.listdir(data_path)\n",
    "file_to_be_tested=[\"arcene_train.data.csv\"]\n",
    "for filename in file_to_be_tested:\n",
    "    dataset = read_csv(data_path+filename, header=None)\n",
    "    data = dataset.values\n",
    "    dimension=data.shape[1] - 1\n",
    "    \n",
    "    k_list= [dimension//5, dimension//4,dimension//3, dimension//2, dimension-1]\n",
    "    k_list=list(set(k_list))\n",
    "    print(\"Tested dataset:\",filename,\"- dimension:\",dimension,\" - k best dimension tested:\",k_list)\n",
    "    for num in k_list:\n",
    "        if num>=1:\n",
    "         acc_baseline_all_features,acc_ars,acc_chi2,acc_mi,acc_relief=test_categorical_dataset(data_path+filename,num)\n",
    "         print(\"all:\",acc_baseline_all_features,\" - ars:\",acc_ars,\" - chi2:\",acc_chi2,\" - mi:\",acc_mi,\" - relief:\",acc_relief)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
